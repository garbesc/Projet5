{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b9d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c76b6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenizer_fct(sentence) :\n",
    "    # print(sentence)\n",
    "    sentence_clean = sentence.replace('-', ' ').replace('+', ' ').replace('/', ' ').\\\n",
    "                             replace('#', ' ').replace('<p>', ' ').replace('>', ' ').replace('<', ' ')\n",
    "    # Remove ponctuation (except # and ++ for c# and c++)\n",
    "    sentence_clean1 = re.sub('[^\\\\w\\\\s#\\\\s++]', ' ', sentence_clean)\n",
    "\n",
    "    # Remove numbers\n",
    "    sentence_clean2 = re.sub(r'\\w*\\d+\\w*', ' ', sentence_clean1)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    sentence_clean = re.sub('\\s+', ' ', sentence_clean2)\n",
    "    word_tokens = word_tokenize(sentence_clean)\n",
    "    return word_tokens\n",
    "\n",
    "# Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_w = list(set(stopwords.words('english'))) + ['[', ']', ',', '.', ':', '?', '(', ')']\n",
    "stop_w.extend(['code', 'quot', 'use', 'http', 'com', 'error', 'work', 'want', 'one', 'would', 'need', \n",
    "                   'help', 'also', 'exampl', 'could', 'thing', 'well', 'dear', 'p'])\n",
    "\n",
    "def stop_word_filter_fct(list_words) :\n",
    "    filtered_w = [w for w in list_words if not w in stop_w]\n",
    "#    filtered_w2 = [w for w in filtered_w if len(w) > 2]\n",
    "    return filtered_w\n",
    "\n",
    "# lower case et alpha\n",
    "def lower_start_fct(list_words) :\n",
    "    lw = [w.lower() for w in list_words if (not w.startswith(\"@\")) \n",
    "                                      and (not w.startswith(\"#\"))\n",
    "                                       and (not w.startswith(\"http\"))]\n",
    "    return lw\n",
    "\n",
    "# Lemmatizer (base d'un mot)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_fct(list_words) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "\n",
    "# Fonction de préparation du texte pour le bag of words (Countvectorizer et Tf_idf, Word2Vec)\n",
    "def transform_bow_fct(desc_text) :\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    # lem_w = lemma_fct(lw)    \n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n",
    "#    return lw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43350fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#file_X_tfidf = open('./models/X_tfidf.pkl', 'rb')\n",
    "#X_tfidf = pickle.load(file_X_tfidf)\n",
    "#file_X_tfidf.close()\n",
    "#file_y_mlb = open('./models/y_mlb.pkl', 'rb')\n",
    "#y_mlb = pickle.load(file_y_mlb)\n",
    "#file_y_mlb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5809a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # mise en forme du texte\n",
    "    text_prep = transform_bow_fct(text)\n",
    "    tt = \"\"\n",
    "    for word in text_prep.split(\" \"):\n",
    "        tt = tt + word + \" \"\n",
    "    text_final = []\n",
    "    text_final.append(tt)\n",
    "    print (text_final)\n",
    "    \n",
    "    # recherche du classifieur pré-entrainé\n",
    "    file_clf = open('./models/clf.pkl', 'rb')\n",
    "    clf = pickle.load(file_clf)\n",
    "       \n",
    "    # recherche du vectoriseur pré-entrainé\n",
    "    file_vect = open('./models/vect.pkl', 'rb')\n",
    "    vect = pickle.load(file_vect)\n",
    "\n",
    "    X_text = vect.transform(text_final)    \n",
    "    y_pred = clf.predict(X_text)\n",
    "    \n",
    "    # recherche du multiLabelBinarizer pré-entrainé\n",
    "    file_mlb = open('./models/mlb.pkl', 'rb')\n",
    "    mlb = pickle.load(file_mlb)\n",
    "\n",
    "    y_pred_inversed = mlb.inverse_transform(y_pred)\n",
    "\n",
    "    return y_pred_inversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31ab88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = \"<p>python how to send pdf or other media as messages to people with WhatsApp click to chat.</p>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1759403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python send pdf media messages people whatsapp click chat ']\n",
      "[()]\n"
     ]
    }
   ],
   "source": [
    "print (process_text(text_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40741299",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep = preprocessor.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b81563",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "366b1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned/df_final_version.csv\",sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8894711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "for col in ['Title', 'Body', 'Tags']:\n",
    "     df[col] = df[col].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6772194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_mlb = mlb.fit_transform(df.Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a610689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\",\n",
    "                             max_df=.97,\n",
    "                             min_df= 3,\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=' '.join,\n",
    "                             stop_words=None,\n",
    "                             lowercase=False)\n",
    "X = df.Body\n",
    "vectorizer.fit(X)\n",
    "X_tfidf = vectorizer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27c998ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_mlb, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4143a96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(dual=False, max_iter=10000))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "final_model = OneVsRestClassifier(LinearSVC(dual=False, max_iter=10000))\n",
    "final_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "455368fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score ROC_AUC :  0.6943480522471015\n",
      "Accuracy :  0.24\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster, metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score\n",
    "y_pred = final_model.predict(X_test)\n",
    "print('Score ROC_AUC : ', metrics.roc_auc_score(y_test, y_pred,multi_class=\"ovr\"))\n",
    "print ('Accuracy : ', round(accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecfcbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_final_model = './models/final_model.pkl'\n",
    "#pickle.dump(final_model, open(file_final_model, 'wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "103c056f",
   "metadata": {},
   "source": [
    "Pré traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04e246a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"register go multiple route french version app web\"\n",
    "text_tok = transform_bow_fct(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9f8181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "register go multiple route french version app web \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['register go multiple route french version app web ']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = \"\"\n",
    "for word in text_tok.split(\" \"):\n",
    "    tt = tt + word + \" \"\n",
    "print (tt)\n",
    "ta = []\n",
    "ta.append(tt)\n",
    "ta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc463ab",
   "metadata": {},
   "source": [
    "Transformation du document en vecteur et prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdbe75c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 26530)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text = vectorizer.transform(ta)\n",
    "X_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ce79f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9845148e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6719025f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[()]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_inversed = mlb.inverse_transform(y_pred)\n",
    "y_pred_inversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73d34b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ecbf395",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc57c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dea6d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "mlflow.sklearn.save_model(final_model, 'mlflow_model', signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1dab7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
